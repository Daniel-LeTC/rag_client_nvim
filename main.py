import argparse
import os
import sys

# Import LangChain & Chroma & Ollama
from langchain_chroma import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_core.documents import Document
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# --- CONFIG ---
NOTE_PATH = "/home/daniel/Projects/mind_dump/"
DB_PATH = "./chroma_db"
LLM_MODEL = "llama3.2:3b"
EMBED_MODEL = "mxbai-embed-large"


def load_and_index(force_rebuild=False):
    """ƒê·ªçc note, bƒÉm nh·ªè v√† nh√©t v√†o Vector DB"""
    if os.path.exists(DB_PATH) and not force_rebuild:
        print(f"‚ö° ƒê√£ t√¨m th·∫•y DB t·∫°i {DB_PATH}. Load l√™n x√†i lu√¥n...")
        return Chroma(persist_directory=DB_PATH, embedding_function=OllamaEmbeddings(model=EMBED_MODEL))

    print("‚ôªÔ∏è  ƒêang qu√©t note v√† t·∫°o index m·ªõi (ch·ªù t√≠ nha bro)...")

    if not os.path.exists(NOTE_PATH):
        print(f"‚ùå ƒê∆∞·ªùng d·∫´n {NOTE_PATH} kh√¥ng t·ªìn t·∫°i!")
        sys.exit(1)

    loader = DirectoryLoader(NOTE_PATH, glob="**/*.md", loader_cls=TextLoader)
    docs = loader.load()

    if not docs:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file .md n√†o ƒë·ªÉ h·ªçc c·∫£!")
        sys.exit(1)

    # Chunk size 500 l√† ƒëi·ªÉm ng·ªçt
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, chunk_overlap=100, add_start_index=True, separators=["\n## ", "\n### ", "\n- ", "\n", " "]
    )
    splits = text_splitter.split_documents(docs)

    vectorstore = Chroma.from_documents(
        documents=splits, embedding=OllamaEmbeddings(model=EMBED_MODEL), persist_directory=DB_PATH
    )
    print(f"‚úÖ ƒê√£ index xong {len(splits)} chunks v√†o Database!")
    return vectorstore


def generate_multi_queries(original_query, llm):
    """
    Bi·∫øn c√¢u h·ªèi ng√°o ng∆° c·ªßa m√†y th√†nh 4 c√¢u h·ªèi s√°t th·ªß (Ti·∫øng Anh + Vi·ªát + Keyword).
    """
    print(f"üß† ƒêang brainstorm ƒë·ªÉ hi·ªÉu √Ω m√†y: '{original_query}'...")

    template = """
    B·∫°n l√† m·ªôt tr·ª£ l√Ω AI gi√∫p t√¨m ki·∫øm th√¥ng tin trong ghi ch√∫ c√° nh√¢n (Second Brain).
    Ghi ch√∫ th∆∞·ªùng ch·ª©a c√°c thu·∫≠t ng·ªØ ti·∫øng Anh (Coding, Biohacks, Workflow) v√† ti·∫øng Vi·ªát.
    
    Nhi·ªám v·ª•: D·ª±a tr√™n c√¢u h·ªèi g·ªëc c·ªßa ng∆∞·ªùi d√πng, h√£y t·∫°o ra 4 phi√™n b·∫£n c√¢u h·ªèi t√¨m ki·∫øm kh√°c nhau ƒë·ªÉ ƒë·∫£m b·∫£o t√¨m th·∫•y th√¥ng tin.
    
    Y√™u c·∫ßu:
    1. Phi√™n b·∫£n 1: D·ªãch sang ti·∫øng Anh (n·∫øu c√¢u g·ªëc l√† Vi·ªát) ho·∫∑c ng∆∞·ª£c l·∫°i.
    2. Phi√™n b·∫£n 2: T·∫≠p trung v√†o t·ª´ kh√≥a chuy√™n ng√†nh (Technical Keywords).
    3. Phi√™n b·∫£n 3: T√¨m c√°c t·ª´ ƒë·ªìng nghƒ©a ho·∫∑c li√™n quan (V√≠ d·ª•: "ng·ªß n√¥ng" -> "insomnia", "sleep quality", "NSDR").
    4. Phi√™n b·∫£n 4: Gi·ªØ nguy√™n c√¢u g·ªëc.
    
    Ch·ªâ tr·∫£ v·ªÅ danh s√°ch c√°c c√¢u h·ªèi ngƒÉn c√°ch b·ªüi d·∫•u xu·ªëng d√≤ng. Kh√¥ng gi·∫£i th√≠ch g√¨ th√™m.
    
    C√¢u h·ªèi g·ªëc: {question}
    """

    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt | llm | StrOutputParser()

    response = chain.invoke({"question": original_query})
    queries = [q.strip() for q in response.split("\n") if q.strip()]

    # In ra ƒë·ªÉ m√†y th·∫•y n√≥ kh√¥n th·∫ø n√†o
    print(f"üîç AI ƒë√£ sinh ra c√°c t·ª´ kh√≥a t√¨m ki·∫øm: {queries}")
    return queries


def chat(query, vectorstore):
    """H·ªèi xo√°y ƒë√°p xoay - Version: Multi-Query Semantic Search"""

    llm = ChatOllama(model=LLM_MODEL, temperature=0)  # Temp th·∫•p ƒë·ªÉ logic ch·∫∑t ch·∫Ω

    # 1. GENERATE QUERIES: ƒê·∫ª ra nhi·ªÅu c√¢u h·ªèi
    generated_queries = generate_multi_queries(query, llm)

    # 2. RETRIEVE: Qu√©t t·∫•t c·∫£ c√°c c√¢u h·ªèi
    # D√πng list comprehension ƒë·ªÉ t√¨m ki·∫øm cho t·ª´ng query
    unique_docs = {}
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})  # M·ªói query l·∫•y 3 k·∫øt qu·∫£

    for q in generated_queries:
        docs = retriever.invoke(q)
        for doc in docs:
            # D√πng n·ªôi dung l√†m key ƒë·ªÉ l·ªçc tr√πng l·∫∑p (Deduplication)
            if doc.page_content not in unique_docs:
                unique_docs[doc.page_content] = doc

    final_docs = list(unique_docs.values())

    if not final_docs:
        print("‚ùå Retriever b√°o: M√†y h·ªèi kh√≥ qu√°, tao brainstorm n√°t √≥c v·∫´n kh√¥ng t√¨m th·∫•y note n√†o kh·ªõp.")
        return

    # 3. ANSWER: T·ªïng h·ª£p th√¥ng tin
    template = """
    M√†y l√† Tr·ª£ l√Ω Second Brain. D·ª±a v√†o Context (ƒë√£ ƒë∆∞·ª£c l·ªçc t·ª´ nhi·ªÅu ngu·ªìn), h√£y tr·∫£ l·ªùi c√¢u h·ªèi.

    LU·∫¨T:
    1. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.
    2. T·ªïng h·ª£p th√¥ng tin t·ª´ c√°c ƒëo·∫°n Context b√™n d∆∞·ªõi.
    3. N·∫øu Context kh√¥ng li√™n quan -> N√≥i "Trong note ch∆∞a ghi".
    4. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, style Coder.

    Context:
    {context}
    
    C√¢u h·ªèi g·ªëc: {question}
    
    Tr·∫£ l·ªùi:
    """
    prompt = ChatPromptTemplate.from_template(template)

    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": lambda x: format_docs(final_docs), "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    print(f"\nü§ñ Polymath Bot ({LLM_MODEL}):")
    print("-" * 30)
    for chunk in rag_chain.stream(query):
        print(chunk, end="", flush=True)
    print("\n" + "-" * 30)

    # --- IN NGU·ªíN (CITATIONS) ---
    print("\nüìÑ NGU·ªíN D·ªÆ LI·ªÜU G·ªêC (T·ªïng h·ª£p t·ª´ Multi-Query):")
    for i, doc in enumerate(final_docs[:5]):  # Ch·ªâ in 5 c√°i ƒë·∫ßu
        source = os.path.basename(doc.metadata.get("source", "Unknown"))
        snippet = doc.page_content.replace("\n", " ")[:100]
        print(f"[{i + 1}] ({source}) ...{snippet}...")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Chat v·ªõi ƒë·ªëng r√°c c·ªßa m√†y")
    parser.add_argument("query", type=str, nargs="?", help="C√¢u h·ªèi")
    parser.add_argument("--rebuild", action="store_true", help="X√≥a DB c≈©, index l·∫°i t·ª´ ƒë·∫ßu")

    args = parser.parse_args()

    vectorstore = load_and_index(force_rebuild=args.rebuild)

    if args.query:
        chat(args.query, vectorstore)
    else:
        while True:
            try:
                user_input = input("\nM√†y (g√µ 'q' ƒë·ªÉ t√©): ")
                if user_input.lower() in ["q", "exit", "quit"]:
                    break
                chat(user_input, vectorstore)
            except KeyboardInterrupt:
                break
