import os
import sys
import warnings

from langchain_chroma import Chroma
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# Import c·∫£ 2 th∆∞ vi·ªán
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_ollama import ChatOllama, OllamaEmbeddings

# Config an to√†n
from config import (
    CLOUD_MODEL_NAME,
    COLLECTION_NAME,
    EMBEDDING_MODEL_NAME,
    GOOGLE_API_KEY,
    LOCAL_MODEL_NAME,
    POLY_SYSTEM_PROMPT,
    VECTOR_DB_PATH,
)

warnings.filterwarnings("ignore")


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# --- H√ÄM KH·ªûI T·∫†O N√ÉO B·ªò (HYBRID) ---
def get_llm(force_local=False):
    """
    ∆Øu ti√™n d√πng Cloud (Gemini). N·∫øu force_local=True ho·∫∑c thi·∫øu Key th√¨ d√πng Local (Qwen).
    """
    if not force_local and GOOGLE_API_KEY:
        try:
            print(f"‚òÅÔ∏è  ƒêang k·∫øt n·ªëi v·ªá tinh Google ({CLOUD_MODEL_NAME})...")
            llm = ChatGoogleGenerativeAI(
                model=CLOUD_MODEL_NAME,
                google_api_key=GOOGLE_API_KEY,
                temperature=0,
                convert_system_message_to_human=True,
            )
            return llm, "CLOUD"
        except Exception as e:
            print(f"‚ö†Ô∏è  L·ªói k·∫øt n·ªëi Cloud: {e}. Chuy·ªÉn sang Local.")

    print(f"üè† ƒêang kh·ªüi ƒë·ªông m√°y ph√°t ƒëi·ªán Local ({LOCAL_MODEL_NAME})...")
    llm = ChatOllama(model=LOCAL_MODEL_NAME, temperature=0, keep_alive="1h")
    return llm, "LOCAL"


def main():
    if not os.path.exists(VECTOR_DB_PATH):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y Database t·∫°i {VECTOR_DB_PATH}!")
        return

    print(f"‚ö° ƒê√£ t√¨m th·∫•y DB t·∫°i {VECTOR_DB_PATH}. Load h√†ng n√≥ng...")

    try:
        # V·∫´n d√πng Local Embedding cho nhanh & r·∫ª
        embedding_function = OllamaEmbeddings(model=EMBEDDING_MODEL_NAME)
        vectorstore = Chroma(
            persist_directory=VECTOR_DB_PATH, embedding_function=embedding_function, collection_name=COLLECTION_NAME
        )
    except Exception as e:
        print(f"üíÄ L·ªói load DB: {e}")
        return

    retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 30})

    print("üß† ƒêang t·∫£i Reranker (CPU Mode)...")
    try:
        model_kwargs = {"device": "cpu"}
        reranker = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base", model_kwargs=model_kwargs)
        print("‚úÖ Reranker ƒë√£ s·∫µn s√†ng.")
    except Exception:
        reranker = None

    # Kh·ªüi t·∫°o n√£o b·ªô l·∫ßn ƒë·∫ßu
    llm, mode = get_llm()
    prompt = ChatPromptTemplate.from_template(POLY_SYSTEM_PROMPT)

    print("\n" + "=" * 40)
    print(f"üí¨ POLYMATH BRO IS ONLINE [{mode} MODE]")
    print("G√µ 'q' ƒë·ªÉ t√©. G√µ 'swap' ƒë·ªÉ ƒë·ªïi ch·∫ø ƒë·ªô Cloud/Local.")
    print("=" * 40)

    while True:
        try:
            query = input("\nM√†y: ").strip()
            if query.lower() in ["q", "quit", "exit"]:
                print("üëã Bye bro.")
                break

            # T√≠nh nƒÉng ·∫©n: Cho ph√©p m√†y t·ª± ƒë·ªïi mode
            if query.lower() == "swap":
                new_mode = not (mode == "CLOUD")  # Toggle
                llm, mode = get_llm(force_local=new_mode)
                print(f"üîÑ ƒê√£ chuy·ªÉn sang ch·∫ø ƒë·ªô: {mode}")
                continue

            if not query:
                continue

            print(f"\nüîç ƒêang b·ªõi th√πng r√°c t√¨m: '{query}'...")

            # --- RAG RETRIEVAL ---
            retrieved_docs = retriever.invoke(query)
            final_docs = []

            # Rerank Logic
            if reranker:
                try:
                    pairs = [[query, doc.page_content] for doc in retrieved_docs]
                    scores = reranker.score(pairs)
                    scored_docs = sorted(zip(retrieved_docs, scores), key=lambda x: x[1], reverse=True)

                    # Threshold l·ªçc nh·∫π (-10.0 l√† l·∫•y g·∫ßn h·∫øt ƒë·ªÉ AI t·ª± l·ªçc)
                    for doc, score in scored_docs[:7]:
                        if score > -10.0:
                            final_docs.append(doc)

                    if not final_docs and scored_docs:
                        final_docs = [scored_docs[0][0]]
                except:
                    final_docs = retrieved_docs[:5]
            else:
                final_docs = retrieved_docs[:5]

            if not final_docs:
                print("\nü§ñ Polymath Bot:")
                print("-" * 30)
                print("Tao ch·ªãu. Kh√¥ng t√¨m th·∫•y th√¥ng tin n√†o kh·ªõp c·∫£.")
                continue

            context_text = format_docs(final_docs)
            chain = prompt | llm | StrOutputParser()

            print(f"\nü§ñ Polymath Bot ({mode}):")
            print("-" * 30)

            # --- TRY/EXCEPT CHO LLM CALL (FALLBACK LOGIC) ---
            try:
                for chunk in chain.stream({"context": context_text, "question": query}):
                    print(chunk, end="", flush=True)
            except Exception as e:
                print(f"\n\n‚ö†Ô∏è  L·ªói khi g·ªçi {mode}: {e}")
                if mode == "CLOUD":
                    print("üîÑ ƒêang chuy·ªÉn sang LOCAL (Qwen) ƒë·ªÉ c·ª©u v√£n t√¨nh th·∫ø...")
                    llm, mode = get_llm(force_local=True)  # Switch to Local
                    # Retry ngay l·∫≠p t·ª©c v·ªõi Local LLM
                    chain = prompt | llm | StrOutputParser()
                    for chunk in chain.stream({"context": context_text, "question": query}):
                        print(chunk, end="", flush=True)
                else:
                    print("üíÄ Local c≈©ng ch·∫øt. M√†y check l·∫°i Ollama ƒëi.")

            print("\n" + "-" * 30)

            # Evidence
            print("üìö Ngu·ªìn d·ªØ li·ªáu (Evidence):")
            seen_sources = set()
            for i, doc in enumerate(final_docs):
                source = os.path.basename(doc.metadata.get("source", "Unknown"))
                if source not in seen_sources:
                    print(f"   [{i + 1}] {source}")
                    seen_sources.add(source)
            print("-" * 30)

        except KeyboardInterrupt:
            print("\nüëã Bye!")
            break
        except Exception as e:
            print(f"\n‚ùå L·ªói h·ªá th·ªëng: {e}")


if __name__ == "__main__":
    main()
