import argparse
import os
import sys

import torch
from langchain_chroma import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_ollama import ChatOllama, OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# Import Reranker Libraries
from sentence_transformers import CrossEncoder

# --- CONFIG ---
NOTE_PATH = "/home/daniel/Projects/mind_dump/"
DB_PATH = "./chroma_db"
LLM_MODEL = "llama3:8b"
EMBED_MODEL = "mxbai-embed-large"
RERANKER_MODEL = "cross-encoder/ms-marco-TinyBERT-L-2"


def load_and_index(force_rebuild=False):
    """ƒê·ªçc note, bƒÉm nh·ªè v√† nh√©t v√†o Vector DB"""
    if os.path.exists(DB_PATH) and not force_rebuild:
        print(f"‚ö° ƒê√£ t√¨m th·∫•y DB t·∫°i {DB_PATH}. Load l√™n x√†i lu√¥n...")
        return Chroma(persist_directory=DB_PATH, embedding_function=OllamaEmbeddings(model=EMBED_MODEL))

    print("‚ôªÔ∏è  ƒêang qu√©t note v√† t·∫°o index m·ªõi (ch·ªù t√≠ nha bro)...")

    if not os.path.exists(NOTE_PATH):
        print(f"‚ùå ƒê∆∞·ªùng d·∫´n {NOTE_PATH} kh√¥ng t·ªìn t·∫°i!")
        sys.exit(1)

    loader = DirectoryLoader(NOTE_PATH, glob="**/*.md", loader_cls=TextLoader)
    docs = loader.load()

    if not docs:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file .md n√†o ƒë·ªÉ h·ªçc c·∫£!")
        sys.exit(1)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,  # K√≠ch th∆∞·ªõc m·ªói mi·∫øng
        chunk_overlap=100,  # G·ªëi ƒë·∫ßu nhau ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh
        add_start_index=True,
        separators=["\n## ", "\n### ", "\n- ", "\n", " "],
    )
    splits = text_splitter.split_documents(docs)

    vectorstore = Chroma.from_documents(
        documents=splits, embedding=OllamaEmbeddings(model=EMBED_MODEL), persist_directory=DB_PATH
    )
    print(f"‚úÖ ƒê√£ index xong {len(splits)} chunks v√†o Database!")
    return vectorstore


def chat(query, vectorstore, reranker):  # <-- TH√äM reranker v√†o tham s·ªë
    """H·ªèi xo√°y ƒë√°p xoay - Version: Sniper Elite"""

    # 1. Qu√©t r·ªông (Retriever - H√∫t b·ª•i)
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 15},  # L·∫•y 15 chunks ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng b·ªè s√≥t c√°i n√†o
    )

    print(f"\nüîç ƒêang b·ªõi th√πng r√°c t√¨m: '{query}'...")

    try:
        raw_docs = retriever.invoke(query)
    except Exception:
        raw_docs = []

    if not raw_docs:
        print("‚ùå Retriever b√°o: Kh√¥ng t√¨m th·∫•y b·∫•t k·ª≥ ƒëo·∫°n n√†o kh·ªõp!")
        retrieved_docs = []
    else:
        # --- FIX 2: L·ªçc Metadata V√î D·ª§NG ---
        filtered_docs = [
            doc
            for doc in raw_docs
            if "AI_METADATA" not in doc.page_content  # L·ªçc c√°c ƒëo·∫°n ch·ªâ ch·ª©a metadata v√¥ d·ª•ng
        ]

        if not filtered_docs:
            print("‚ùå L·ªçc Metadata: Kh√¥ng c√≤n n·ªôi dung h·ªØu √≠ch n√†o ƒë·ªÉ rerank!")
            retrieved_docs = []
        else:
            # 2. Rerank (L·ªçc c√°t ƒë√£i v√†ng)

            # T·∫°o c·∫∑p [query, doc.page_content]
            sentence_pairs = [[query, doc.page_content] for doc in filtered_docs]

            # Ch·∫•m ƒëi·ªÉm
            scores = reranker.predict(sentence_pairs)

            # Gh√©p ƒëi·ªÉm v√†o doc v√† s·∫Øp x·∫øp
            scored_docs = sorted(
                [(score, doc) for score, doc in zip(scores, filtered_docs)], key=lambda x: x[0], reverse=True
            )

            # 3. L·∫•y TOP 5 CH·∫§T L∆Ø·ª¢NG NH·∫§T (Output cho LLM)
            # Ch·ªâ l·∫•y 5 c√°i c√≥ ƒëi·ªÉm Reranker cao nh·∫•t
            retrieved_docs = [doc for score, doc in scored_docs[:5]]

    if not retrieved_docs:
        # N·∫øu ƒë√£ qua Reranker m√† v·∫´n kh√¥ng c√≥ g√¨, th√¨ b√°o l·ªói.
        print("‚ùå Reranker/Filter lo·∫°i h·∫øt v√¨ kh√¥ng c√≥ ƒëo·∫°n n√†o li√™n quan (ho·∫∑c ƒëi·ªÉm qu√° th·∫•p)!")

    # 4. Setup LLM
    llm = ChatOllama(model=LLM_MODEL, temperature=0.1)

    # 5. PROMPT: Gi·ªØ nguy√™n prompt V3 ƒë√£ s·ª≠a
    template = """
    M√†y l√† Tr·ª£ l√Ω Second Brain th√¥ng minh. M√†y ph·∫£i tr·∫£ l·ªùi ch√≠nh x√°c, m·∫°ch l·∫°c.
    
    QUY T·∫ÆC B·∫§T KH·∫¢ X√ÇM PH·∫†M:
    1. B·∫Øt bu·ªôc d√πng TI·∫æNG VI·ªÜT ƒë·ªÉ tr·∫£ l·ªùi.
    2. CH·ªà s·ª≠ d·ª•ng c√°c th√¥ng tin n·∫±m trong ph·∫ßn "Context" b√™n d∆∞·ªõi.
    3. Tr·∫£ l·ªùi b·∫±ng c√°ch T·ªîNG H·ª¢P v√† DI·ªÑN GI·∫¢I l·∫°i n·ªôi dung.
    4. Tr·∫£ l·ªùi d∆∞·ªõi d·∫°ng G·∫†CH ƒê·∫¶U D√íNG.
    5. N·∫øu Context kh√¥ng c√≥ b·∫•t k·ª≥ th√¥ng tin n√†o li√™n quan -> Tr·∫£ l·ªùi ng·∫Øn g·ªçn: "Th√¥ng tin n√†y ch∆∞a ƒë∆∞·ª£c ghi l·∫°i trong c√°c note c·ªßa m√†y."

    Context:
    {context}
    
    C√¢u h·ªèi: {question}
    
    Tr·∫£ l·ªùi:
    """
    prompt = ChatPromptTemplate.from_template(template)

    # 6. Chain
    def format_docs(docs):
        return "\n\n".join(doc.page_content for doc in docs)

    rag_chain = (
        {"context": lambda x: format_docs(retrieved_docs), "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    # 7. Run & Print Answer
    print(f"\nü§ñ Polymath Bot ({LLM_MODEL}):")
    print("-" * 30)

    for chunk in rag_chain.stream(query):
        print(chunk, end="", flush=True)
    print("\n" + "-" * 30)

    # --- IN NGU·ªíN (CITATIONS) ---
    print("\nüìÑ NGU·ªíN D·ªÆ LI·ªÜU G·ªêC (ƒê√£ Rerank):")
    if retrieved_docs:
        for i, doc in enumerate(retrieved_docs):
            source = doc.metadata.get("source", "Unknown")
            snippet = doc.page_content.replace("\n", " ")[:100]
            filename = os.path.basename(source)
            print(f"[{i + 1}] ({filename}) ...{snippet}...")
    else:
        print("(Kh√¥ng t√¨m th·∫•y ngu·ªìn n√†o kh·ªõp)")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Chat v·ªõi ƒë·ªëng r√°c c·ªßa m√†y")
    parser.add_argument("query", type=str, nargs="?", help="C√¢u h·ªèi")
    parser.add_argument("--rebuild", action="store_true", help="X√≥a DB c≈©, index l·∫°i t·ª´ ƒë·∫ßu")

    args = parser.parse_args()

    # Load DB
    vectorstore = load_and_index(force_rebuild=args.rebuild)

    # --- FIX 1: Kh·ªüi t·∫°o Reranker 1 L·∫¶N duy nh·∫•t ---
    print("üß† ƒêang t·∫£i Reranker (ch·ªâ 1 l·∫ßn)...")
    try:
        # N·∫øu model ƒë√£ t·∫£i v·ªÅ, n√≥ s·∫Ω kh·ªüi t·∫°o r·∫•t nhanh
        reranker = CrossEncoder(RERANKER_MODEL)
        print("‚úÖ Reranker ƒë√£ s·∫µn s√†ng.")
    except Exception as e:
        print(f"‚ùå L·ªói t·∫£i Reranker: {e}. Vui l√≤ng ki·ªÉm tra uv add sentence-transformers torch.")
        sys.exit(1)

    # Chat logic
    if args.query:
        chat(args.query, vectorstore, reranker)  # <-- TH√äM reranker v√†o l·ªánh g·ªçi
    else:
        while True:
            try:
                user_input = input("\nM√†y (g√µ 'q' ƒë·ªÉ t√©): ")
                if user_input.lower() in ["q", "exit", "quit"]:
                    break
                chat(user_input, vectorstore, reranker)  # <-- TH√äM reranker v√†o l·ªánh g·ªçi
            except KeyboardInterrupt:
                break
